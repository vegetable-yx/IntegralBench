# Supplementary

- dataset.jsonl: The main dataset used for evaluation, containing the integral problems and their corresponding ground-truth answers.

- eval_prompts.yaml: The standardized prompt templates used to query large language models (LLMs) during inference.

- eval_output_jsonl: The parsed outputs from LLMs after inference, including symbolic and numerical answers extracted from raw responses.

- symbolic_evaluation/: Contains the Python scripts generated by LLMs to verify symbolic expressions.

- check_analy.yaml: Prompt configurations used specifically for verifying symbolic correctness through analytical methods.

- test_cases/: The original raw LLM outputs before parsing, serving as the unprocessed results of model inference.

- acc_num/: Evaluation results of numerical accuracy for each model across all individual rounds.

- acc_symbolic/: Evaluation results of symbolic accuracy for each model across individual rounds.

- acc_all@3/: Aggregated symbolic and numerical accuracy results under the All@3 setting.

- acc_pass@3/: Aggregated results for symbolic and numerical correctness under the Pass@3 setting.
